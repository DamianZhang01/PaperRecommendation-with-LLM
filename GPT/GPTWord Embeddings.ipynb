{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89352281-16b9-4a97-8f30-22226d7f4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from random import randrange\n",
    "# from tqdm.notebook import tqdm_no\n",
    "from IPython.display import display, Markdown\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1b967e-4603-4271-8f7c-790e4085b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your corresponding key to download dataset from Kaggle\n",
    "# https://www.kaggle.com/datasets/Cornell-University/arxiv\n",
    "\n",
    "with open('./kaggle.json') as f:\n",
    "    file = json.load(f)\n",
    "\n",
    "os.environ[\"KAGGLE_KEY\"] = file['key']\n",
    "os.environ[\"KAGGLE_USERNAME\"] = 'jeploretizo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d645ae8-9cec-40b1-ae2c-297226f294a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d Cornell-University/arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e854e317-1889-4f27-836b-4905bca05d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  arxiv.zip\n",
      "  inflating: arxiv-metadata-oai-snapshot.json  \n"
     ]
    }
   ],
   "source": [
    "! unzip \"arxiv.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e096305d-c90a-447e-a728-7c7f2bcbfd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "FILE_PATH ='./arxiv-metadata-oai-snapshot.json'\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25208d6e-4774-4711-bb9e-aec95dfd2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    with open(FILE_PATH) as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d73cb1c-3ffe-4dd2-bf42-2961d06e1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_limit = 2023\n",
    "\n",
    "dataframe = {\n",
    "    'id': [],\n",
    "    'title': [],\n",
    "    'year': [],\n",
    "    'abstract': []\n",
    "\n",
    "}\n",
    "\n",
    "data = get_data()\n",
    "for i, paper in enumerate(data):\n",
    "    paper = json.loads(paper)\n",
    "    try:\n",
    "        date = int(paper['update_date'].split('-')[0])\n",
    "        if date > year_limit:\n",
    "            dataframe['title'].append(paper['title'])\n",
    "            dataframe['year'].append(date)\n",
    "            dataframe['abstract'].append(paper['abstract'])\n",
    "            dataframe['id'].append(paper['id'])\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f21cfa9-bf0e-4b2c-a097-a267c993f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.DataFrame(dataframe)\n",
    "\n",
    "# Limit to first 500 for training purposes\n",
    "df = main_df[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c128041d-d5ce-4156-a624-bfbdea6cd263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f26ea09-a3f5-45b7-a2f9-acd8bcd8e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PapersDataset(Dataset):\n",
    "    def __init__(self, tokenizer, titles, abstracts, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.attn_masks = []\n",
    "        for title, abstract in zip(titles, abstracts):\n",
    "            # Concatenate title and abstract with a delimiter\n",
    "            text = f\"Title: {title} Abstract: {abstract}\"\n",
    "            encodings_dict = tokenizer(text, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.inputs.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.attn_masks[idx]\n",
    "\n",
    "titles = df['title'].values\n",
    "abstracts = df['abstract'].values\n",
    "\n",
    "dataset = PapersDataset(tokenizer, titles, abstracts)\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "728d1c22-d9c4-4eb2-b694-7c785112dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92506f96-483a-4807-874e-96435f984cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for epoch in range(50): \n",
    "    model.train()\n",
    "    for inputs, masks in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        outputs = model(inputs, attention_mask=masks, labels=inputs)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} finished\")\n",
    "\n",
    "model.save_pretrained('./my_fine_tuned_model')\n",
    "tokenizer.save_pretrained('./my_fine_tuned_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "508172f7-4786-4f00-8d13-4f250b1f65a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('./my_fine_tuned_model')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('./my_fine_tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11060b9b-58ba-4fd2-9c8d-640e0d0a65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, config={'max_length':512})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cf98b2c-4a3a-4983-b198-811a0a2eb635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_recommendation(model, tokenizer, query, max_length=512):\n",
    "  \n",
    "    inputs = tokenizer.encode(\"Title: \" + query + \" Abstract:\", return_tensors=\"pt\")\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=1.0,  # Adjust temperature for creativity\n",
    "        top_k=50,         # Adjust top_k for diversity\n",
    "        top_p=0.95,       # Adjust top_p for diversity\n",
    "        repetition_penalty=1.2,  # Adjust repetition penalty to discourage repetition\n",
    "        do_sample=True,   # Enable sampling to generate diverse recommendations\n",
    "        num_return_sequences=5  # Number of recommendations to generate\n",
    "    )\n",
    "    \n",
    "    # Decode the generated sequences to text\n",
    "    recommendations = [tokenizer.decode(generated_sequence, skip_special_tokens=True) for generated_sequence in output_sequences]\n",
    "    \n",
    "    return recommendations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86b0fc3e-0d6b-4a93-9039-ffd8b9034352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: machine learning Abstract:  A detailed exposition of neural networks is presented. Each network constructs a training set, and\n",
      "learns new representations such that any associated memory pool accumulates. The training sets are organized into layers or filters\n",
      "whose primary aim lies within the network layer itself; in this paper we concentrate on those modules whose function it (i) Is to Understand Neural Networks by First-Generation\n",
      "Deep Learning Techniques (DLG); (ii), (iii)-(iv)/(v)+ denotes an autonomous process within them which automatically executes further steps when\n",
      "necessary (\"incomplete\" teaching methods like image labeling may be more appropriate). It then transforms the residual data from each filter into its own object so as never again does every single algorithm\n",
      "backwardly miss information from all outputs created through these previously differentiated models take advantage\n",
      "Title: machine learning Abstract: Â The development of algorithms for classification and analysis of problem\n",
      "problems. Most popular are based on automorphisms whose aim is to greatly reduce the complexity, but\n",
      "sometimes require more computational resources. In this paper we consider machine Learning (ML), a new approach which transforms many\n",
      "learning problems into tasks where multiple classes converge according to input data in terms either best or worst\n",
      "of\n",
      "previously predicted future outcomes; therefore ML can be considered as an extension over\n",
      "traditional human-machine collaboration methods such by having higher processing power and reduced\n",
      "constraints. These results show that computing complicated datasets with little or no effort\n",
      "can efficiently perform computationally intensive coding steps without losing fundamental features such depth nor\n",
      "surroundings etc., while still allowing significant abstractions like vector machines, colour\n",
      "fields/rectangles (\"DLR\") tree structures involving linear functions within each\n",
      "element being represented. The drawback of such approaches is often negligible compared against other\n",
      "algorithms due solely to their numerical dimensions at the training stage. The algorithmic efficiency\n",
      "performance of current versions also depend upon various assumptions including model selection beyond latent\n",
      "matrices trained through previous experiments run during the course time taking\n",
      "ML benchmarking.\n",
      "\n",
      "Title: machine learning Abstract:  In this paper we introduce a neural network model to describe the\n",
      "learning of classification problems. The main goal is twofold - firstly, because our model\n",
      "describes exactly how well an algorithm generalizes itself according towards solving its\n",
      "convex tasks; secondly, it collects information about features that are unique across all\n",
      "machine intelligence classes and categorisations them into subgroups based on\n",
      "their input. Classification algorithms naturally classify complex data so they can be refined with suitable training methods if necessary for certain\n",
      "solutions such as matching. Deep learning processes recognise representations from the original source in termsof their readjustment properties against random noise\n",
      "and perform optimisations which keep those optimised performance improvements within tolerance between transformations under appropriate memory\n",
      "framework.\n",
      "\n",
      "Title: machine learning Abstract:  We introduce and discuss three fundamental aspects of machine learning, namely decomposable, latent\n",
      "learning methods that aim at solving a variety-valued problem (say, estimating the number or its size) in terms only about\n",
      "the input data. Representing these methods as generalization steps would be costly to implement in a standardized\n",
      "framework. Instead we propose an algorithm which is based on reinforcement learning theory rather than traditional linear time\n",
      "decomposition approaches such was previously attempted for humans. This iteration enhances existing algorithms with greater efficiency by reducing resource\n",
      "demands and improving computational complexity by mitigating potential side effects such intraplanarity\n",
      "and variability. The reduced processing costs associated up close are almost nil compared against analogous human\n",
      "algorithms trained according Tochigi's model. Nonetheless they provide superior training metrics when utilized optimally; here again our proposal addresses those\n",
      "concerns through reductionism over computation while still retaining key features including predictive power within current constraints where desired information may\n",
      "not have been realized before execution begins taking place across all neural networks. Finally this represents another new\n",
      "approach towards minimising long-term intelligence challenges posed by nonlinear\n",
      "superintelligence tasks like classification and inference. Our proposal aligns best practices with recent\n",
      "accelerated state models specifically targeting multiple objective measures whereas fundamentally reversing\n",
      "this approach reduces computational cost thereby greatly simplifying processes. Keywords include deep learning techniques comprising deconvolutionality\n",
      "of prior outputs, iterative stochasticity, orphan filters and robustness via combining disparate\n",
      "measures. In summary results relating respectively cognitive outcomes arising from previous artificial\n",
      "intrinsicities under consideration versus past representations constitute one common\n",
      "method of discovering and maximizing cognition.\n",
      "\n",
      "Title: machine learning Abstract:  A neural network is a flexible representation of the human visual system that\n",
      "empirically deduces and analyses attributes, values, behaviors, traits as input to the computation\n",
      "process. It is generally well-suited for teaching tasks or simulations. However its limitations often affect how commonly these models are used in\n",
      "study or application science (for instance estimating functional statistics via real data). With limited access at this time it has been proposed That artificial intelligence should be able to do better than other \"nature\"\n",
      "intelligence such an equivalent might be known within the next few decades. Therewith there would exist humans or machines trained on par with nature; however when applied more sophisticated\n",
      "intelligent systems can benefit from their training capacities far beyond those currently found along all dimensions apart cognitively speaking/\n",
      "retrograde. These problems are then further complicated by higher dimensionality requiring different methods like deeplearning /\n",
      "supervised learning etc. Thus our goal was never just To replace one kind of intelligence but rather TO eliminate the\n",
      "from among them entirely. Our approach towards mitigating complexity will involve several key components: firstly we must define what types of\n",
      "assumptions and requirements are implicit in terms two dimensions reasoning while secondly whether they hold independently over multidimensional space\n",
      "sovereigns vis alia. Since various researchers have emphasised explicitly which categories people need before deciding to take up\n",
      "the challenge of solving new knowledge challenges society still naturally generates certain assumptions about reality where\n",
      "therefore are mostly unsuitable solutions outside. We address these questions in particular through constructing representations for basic building blocks\n",
      "such possibly language structures e_n(f) - i.e., functions defined ontologically according t'and f + 1 respectively mapping between fields o and r each\n",
      "having maximal c_{t}=2$. In doing so both we arrive very close indeed allowing inference techniques derived either\n",
      "directly from scratch using classical algorithms optimising original outputs whereas any resulting solution uses some\n",
      "extra extra software code adapted from scratch thereby making possible future improvements including machine learning. This, together witting\n",
      "applications with corresponding computational energy cost helps ensure smarter decisions away\n"
     ]
    }
   ],
   "source": [
    "query = \"machine learning\"  # Example query\n",
    "recommendations = generate_recommendation(model, tokenizer, query)\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8faee32d-baec-4a0f-a716-17f331933ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_titles_from_recommendations(recommendations):\n",
    "    titles = []\n",
    "    for rec in recommendations:\n",
    "        # Split the recommendation into title and abstract parts\n",
    "        parts = rec.split(\" Abstract:\")\n",
    "        title_part = parts[0]  # This part contains \"Title: [Generated Title]\"\n",
    "        \n",
    "        # Further split to isolate the title\n",
    "        title = title_part.replace(\"Title: \", \"\").strip()\n",
    "        titles.append(title)\n",
    "    \n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e63bd18c-0f07-48bc-ac43-490ef7da72dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning\n",
      "machine learning\n",
      "machine learning\n",
      "machine learning\n",
      "machine learning\n"
     ]
    }
   ],
   "source": [
    "recommendations = generate_recommendation(model, tokenizer, \"machine learning\")\n",
    "titles = extract_titles_from_recommendations(recommendations)\n",
    "\n",
    "for title in titles:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1e66768-35e3-4aee-aba3-8ddea9948dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation 1: Title: Causal Inference Abstract:   Theorem. - Theorem. - Theorem. - Let a family of finite groupoids be generated\n",
      "of finite groupoids in a suitable Bayesian space, e.g. from the\n",
      "morphism of the groupoid $A(k)$ to the holomorphic integer $x^q(k)$, and every\n",
      "for each subgroupoid $q$ of $k the algorithm returns the results\n",
      "in a straight forward, Bayesian fashion.\n",
      "\n",
      "\n",
      "Recommendation 2: Title: Causal Inference Abstract:   In this article, we prove that there are three\n",
      "directions to the convex graph from positive probability to Gaussian probability: a) the\n",
      "decomposition of a convex graph by its points of interest; b) the identification\n",
      "of a single point by its coefficients of the convex graph. In this spirit,\n",
      "we give a general proof that the general probability formula ${ \\cal{N}\n",
      "\\cap \\limits_{{{\\omega}(x_{i+1}\\vee i+1})}{I}{{-}$ is correct for all\n",
      "points on the convex graph if and only if their products (the sums\n",
      "of sums) of reduced by the points of interest) are derived. As\n",
      "obtained from the results of the previous two classes of combinatorial\n",
      "informations, we establish that the fact that $\\cal{N}^d^{n-1}(\n",
      "\n",
      "Recommendation 3: Title: Causal Inference Abstract:   Given an object a and a suitable covariant, we prove the existence\n",
      "of an independent positive probabilistic inference function. We prove,\n",
      "in the affirmative, that the method of selection in the classical probabilistic\n",
      "distribution has sufficient parameters to guarantee the existence of the\n",
      "iterated sum function.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Causal Inference\"  # Your query here\n",
    "recommendations = generate_recommendations(query)\n",
    "\n",
    "for idx, rec in enumerate(recommendations, 1):\n",
    "    print(f\"Recommendation {idx}: {rec}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b1432-43df-4965-9b1c-f5c455b1b148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ba8cbc6-1b12-464d-ae22-f216a2002f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Title: Causal Inference Abstract: The Feature Selection Library (FSLib) signifies a notable progression in\\nmachine learning and data mining for MATLAB users, emphasizing the critical\\nrole of Feature Selection (FS) in enhancing model efficiency and effectiveness\\nby pinpointing essential features for specific tasks. FSLib's contributions are\\ncomprehensive, tackling various FS challenges. It offers a wide array of FS\\nalgorithms, including filter, embedded, and wrapper methods, allowing for efficient\\nfeature selection. Filter fundamental features are: multi-selectivity, highlight actions,\\nresponsive element, and wrapper methods that aggregate key performance indicators. Filter\\nmethods prioritize intrinsic feature properties, embedded methods integrate\\nselection within the training process, and wrapper methods evaluate features\\nbased on model performance, catering to diverse modeling approaches. FSLib also\\naddresses the curse of dimensionality by facilitating the selection of relevant\\nfeature subsets, thereby reducing data dimensionality, lessening computational\\n\",\n",
       " \"Title: Causal Inference Abstract: This paper starts with a biographical sketch of the life of Josef Meixner.\\nThen his motivations to work on orthogonal polynomials and special functions\\nare reviewed. Meixner's 1934 paper introducing the Meixner and\\nMeixner-Pollaczek polynomials is discussed in detail. Truksa's forgotten 1931\\npaper, which already contains the Meixner polynomials, is mentioned. The paper\\nends with a survey of the reception of Meixner's 1934 paper.\\n\",\n",
       " \"Title: Causal Inference Abstract: Recent advances in the Langlands program shed light on a vast area of modern\\nmathematics from an unconventional viewpoint, including number theory, gauge\\ntheory, representation, knot theory and etc. By applying to physics, these\\nnovel perspectives endow with a unified account of the (integer/ fractional)\\nquantum Hall effect. The plateaus of the Hall conductance are described by\\nHecke eigensheaves of the geometric Langlands correspondence. Especially, the\\nparticle-vortex duality, which is explained by S-duality of Chern-Simons\\ntheory, corresponds to the Langlands duality in Wilson and Hecke operators.\\nMoreover the Langlands duality in the quantum group associated with the\\nHamiltonian describes fractal energy spectrum structure, know as Hofstadter's\\nbutterfly. These results suggest that the Langlands program has many physically\\nrealistic meanings.\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2d1cc-e406-4b7a-8f81-3d81d39d81a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3214ec2c-42d1-456f-a198-5db36c8041ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
