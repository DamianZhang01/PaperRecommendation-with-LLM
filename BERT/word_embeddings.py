# -*- coding: utf-8 -*-
"""Word Embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f7AcmA8T-6zw_Y0P67uHPiOo0ZQFuhXp
"""

import torch
import random
import json
import os

import numpy as np
import pandas as pd
import torch.nn.functional as F

from random import randrange
from tqdm.notebook import tqdm
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertModel, BertForNextSentencePrediction, AutoTokenizer

# Change to your corresponding key to download dataset from Kaggle
# https://www.kaggle.com/datasets/Cornell-University/arxiv

from google.colab import userdata
import os

os.environ["KAGGLE_KEY"] = userdata.get('KAGGLE_KEY')
os.environ["KAGGLE_USERNAME"] = userdata.get('KAGGLE_USERNAME')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

!kaggle datasets download -d Cornell-University/arxiv

! unzip "arxiv.zip"

PRE_TRAINED_MODEL_NAME = 'bert-base-cased'
MAX_SEQUENCE_LENGTH = 256
FILE_PATH ='./arxiv-metadata-oai-snapshot.json'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def get_data():
    with open(FILE_PATH) as f:
        for line in f:
            yield line

year_limit = 2023

dataframe = {
    'id': [],
    'title': [],
    'year': [],
    'abstract': []

}

data = get_data()
for i, paper in enumerate(data):
    paper = json.loads(paper)
    try:
        date = int(paper['update_date'].split('-')[0])
        if date > year_limit:
            dataframe['title'].append(paper['title'])
            dataframe['year'].append(date)
            dataframe['abstract'].append(paper['abstract'])
            dataframe['id'].append(paper['id'])
    except: pass

df = pd.DataFrame(dataframe)

subset_df = df.iloc[:500]

model = BertForNextSentencePrediction.from_pretrained(PRE_TRAINED_MODEL_NAME)
model = model.to(device)
model.eval()

tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)

def get_similarity_scores(query_string, reference_list, BATCH_SIZE=16):
  similarity_scores = []
  paired_texts = [(query_string, reference_row) for reference_row in reference_list]

  for i in tqdm(range(0, len(paired_texts), BATCH_SIZE)):
      batch = paired_texts[i:i + BATCH_SIZE]
      encoded_sequences = tokenizer(
          [pair[0] for pair in batch],
          [pair[1] for pair in batch],
          padding='longest',
          truncation='longest_first',
          return_tensors='pt',
          max_length=MAX_SEQUENCE_LENGTH
      ).to(device)

      outputs = model(
          input_ids=encoded_sequences['input_ids'],
          attention_mask=encoded_sequences['attention_mask'],
          token_type_ids=encoded_sequences['token_type_ids']
      )

      probs = F.softmax(outputs.logits, dim=1)
      similarity_scores.extend(probs[:, 0].detach().cpu().numpy())

  return similarity_scores

# Assuming similarity_scores is now a list of similarity scores,
# one for each row in your original DataFrame:

query_string = 'credit card access'
df['similarity_score'] = get_similarity_scores(query_string, df['abstract'].to_list())

df = df.sort_values(by='similarity_score', ascending=False).reset_index(drop=True)







for i in range(5):
    row = df.iloc[i]
    print("TITLE:", f"{row['title']}")
    print('----')
    print("ABSTRACT:", row['abstract'])
    print('----')
    print("SIMILARIY SCORE:", row['similarity_score'])
    print('====')



























